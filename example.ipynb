{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ac0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d533481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import init_env_vars\n",
    "\n",
    "# Add the cell_generator module to path\n",
    "sys.path.insert(0, os.path.join(os.environ['REPO_LOCAL_PATH'], 'cell_generator'))\n",
    "\n",
    "# Import cell_generator modules\n",
    "from dataset import DataGen\n",
    "import global_vars as gv\n",
    "from mg_analyzer import analyze_th\n",
    "from utils import *\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set:\n",
      "  REPO_LOCAL_PATH: /home/lionb\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DATA_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment variables set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  REPO_LOCAL_PATH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREPO_LOCAL_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  DATA_PATH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATA_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  MODELS_PATH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODELS_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/maskinterpreter/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATA_PATH'"
     ]
    }
   ],
   "source": [
    "# Set environment variables for the notebook session\n",
    "os.environ['REPO_LOCAL_PATH'] = \"/home/lionb\"\n",
    "os.environ['DATA_PATH'] = \"/mnt/new_groups/assafza_group/assafza/lion_models_clean/example_data/train_test_list/\"\n",
    "os.environ['MODELS_PATH'] = \"/mnt/new_groups/assafza_group/assafza/lion_models_clean/models/\"\n",
    "\n",
    "print(\"Environment variables set:\")\n",
    "print(f\"  REPO_LOCAL_PATH: {os.environ['REPO_LOCAL_PATH']}\")\n",
    "print(f\"  DATA_PATH: {os.environ['DATA_PATH']}\")\n",
    "print(f\"  MODELS_PATH: {os.environ['MODELS_PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run replace_dir.sh to update CSV file paths\n",
    "# !cd ${REPO_LOCAL_PATH}/cell_generator && bash replace_dir.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "data_csv_path = os.path.join(os.environ.get('DATA_PATH'), 'Nuclear-envelope/image_list_test.csv')\n",
    "model_path = os.path.join(os.environ.get('MODELS_PATH'), 'mg_model_ne_13_05_24_1.0')\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Data CSV exists: {os.path.exists(data_csv_path)}\")\n",
    "print(f\"Model path exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "# Load the CSV to check data\n",
    "data_df = pd.read_csv(data_csv_path)\n",
    "print(f\"\\nDataset shape: {data_df.shape}\")\n",
    "print(f\"Dataset columns: {data_df.columns.tolist()}\")\n",
    "print(f\"First few rows:\\n{data_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420933a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override global variables for Nuclear-envelope organelle\n",
    "gv.organelle = \"Nuclear-envelope\"\n",
    "gv.model_path = model_path\n",
    "gv.test_ds_path = data_csv_path\n",
    "\n",
    "# Set input and target channels\n",
    "gv.input = \"channel_signal\"\n",
    "gv.target = \"channel_target\"\n",
    "\n",
    "# Set model parameters\n",
    "gv.model_type = \"MG\"\n",
    "gv.batch_size = 4\n",
    "gv.patch_size = (32, 128, 128, 1)\n",
    "\n",
    "# Display the updated global variables\n",
    "print(\"Updated global variables:\")\n",
    "print(f\"  organelle: {gv.organelle}\")\n",
    "print(f\"  model_path: {gv.model_path}\")\n",
    "print(f\"  model_type: {gv.model_type}\")\n",
    "print(f\"  batch_size: {gv.batch_size}\")\n",
    "print(f\"  patch_size: {gv.patch_size}\")\n",
    "print(f\"  input: {gv.input}\")\n",
    "print(f\"  target: {gv.target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aabb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from CSV file\n",
    "print(f\"Creating dataset from: {data_csv_path}\")\n",
    "print(f\"Using input column: '{gv.input}' and target column: '{gv.target}'\")\n",
    "\n",
    "# Create DataGen object for the test dataset\n",
    "test_dataset = DataGen(\n",
    "    gv.test_ds_path,\n",
    "    input_col=gv.input,\n",
    "    target_col=gv.target,\n",
    "    batch_size=gv.batch_size,\n",
    "    num_batches=32,\n",
    "    patch_size=gv.patch_size,\n",
    "    min_precentage=0.0,\n",
    "    max_precentage=1.0,\n",
    "    augment=False,\n",
    "    norm_type=\"std\",\n",
    "    delete_cahce=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained MaskInterpreter model\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "try:\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Model summary:\")\n",
    "    model.summary()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analyze_th to generate predictions and importance masks\n",
    "# This function will:\n",
    "# 1. Generate predictions on original images\n",
    "# 2. Generate importance masks\n",
    "# 3. Apply thresholding at different levels\n",
    "# 4. Compute mask efficacy (PCC between predictions)\n",
    "# 5. Save visualizations and results\n",
    "\n",
    "print(\"Running analyze_th for predictions and importance mask generation...\")\n",
    "print(\"This may take a while depending on the dataset size...\")\n",
    "\n",
    "# Use only first 2 images for demonstration (use range(len(test_dataset.list_of_image_keys)) for all)\n",
    "num_images_to_analyze = 2\n",
    "images_to_analyze = range(num_images_to_analyze)\n",
    "\n",
    "try:\n",
    "    analyze_th(\n",
    "        dataset=test_dataset,\n",
    "        mode=\"regular\",  # Mode options: \"agg\", \"loo\", \"mask\", \"regular\"\n",
    "        manual_th=\"full\",  # Full importance mask without thresholding\n",
    "        save_image=True,  # Save visualizations\n",
    "        save_histo=False,  # Save importance mask histograms\n",
    "        weighted_pcc=False,  # Use regular PCC (not weighted)\n",
    "        model_path=model_path,\n",
    "        model=model,\n",
    "        compound=None,  # No compound/drug perturbation\n",
    "        images=images_to_analyze,  # Which images to analyze\n",
    "        noise_scale=1.0,  # Noise scale for perturbation\n",
    "        save_results=True,\n",
    "        results_save_path=None  # Will save in model_path/predictions/\n",
    "    )\n",
    "    print(\"analyze_th completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during analyze_th: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daa6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize results from saved predictions\n",
    "def auto_balance(image):\n",
    "    \"\"\"Auto balance the image similar to ImageJ's Auto Contrast function.\"\"\"\n",
    "    # Convert to float32 to avoid issues with overflow/underflow\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    # Calculate the 0.1th and 99.9th percentiles\n",
    "    plow, phigh = np.percentile(image, (0.1, 99.9))\n",
    "    \n",
    "    # Stretch the values to the full range [0, 1]\n",
    "    image = np.clip((image - plow) / (phigh - plow), 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def visualize_results(model_path, image_index=0, z_slice=None):\n",
    "    \"\"\"\n",
    "    Load and visualize saved prediction results from model/predictions directory.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model directory\n",
    "        image_index: Index of the image to visualize\n",
    "        z_slice: Specific z-slice to display (None for middle slice)\n",
    "    \"\"\"\n",
    "    from cell_imaging_utils.image.image_utils import ImageUtils\n",
    "    \n",
    "    predictions_dir = f\"{model_path}/predictions/{image_index}\"\n",
    "    \n",
    "    # Find prediction files for the specified image\n",
    "    input_file = f\"{predictions_dir}/input_{image_index}.tiff\"\n",
    "    target_file = f\"{predictions_dir}/target_{image_index}.tiff\"\n",
    "    pred_file = f\"{predictions_dir}/unet_prediction_{image_index}.tiff\"\n",
    "    mask_file = f\"{predictions_dir}/full/mask_{image_index}.tiff\"\n",
    "    noisy_input_file = f\"{predictions_dir}/full/noisy_input_{image_index}.tiff\"\n",
    "    noisy_pred_file = f\"{predictions_dir}/full/noisy_unet_prediction_{image_index}.tiff\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(pred_file):\n",
    "        print(f\"Prediction file not found: {pred_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load TIFF arrays using ImageUtils\n",
    "    print(\"Loading images...\")\n",
    "    input_image = ImageUtils.imread(input_file) if os.path.exists(input_file) else None\n",
    "    target_image = ImageUtils.imread(target_file) if os.path.exists(target_file) else None\n",
    "    pred_image = ImageUtils.imread(pred_file)\n",
    "    mask_image = ImageUtils.imread(mask_file) if os.path.exists(mask_file) else None\n",
    "    noisy_input_image = ImageUtils.imread(noisy_input_file) if os.path.exists(noisy_input_file) else None\n",
    "    noisy_pred_image = ImageUtils.imread(noisy_pred_file) if os.path.exists(noisy_pred_file) else None\n",
    "    \n",
    "    print(f\"Loaded prediction shape: {pred_image.shape}\")\n",
    "    if mask_image is not None:\n",
    "        print(f\"Loaded mask shape: {mask_image.shape}\")\n",
    "    \n",
    "    # Ensure images have channel dimension\n",
    "    if len(pred_image.shape) == 3:\n",
    "        pred_image = pred_image[..., np.newaxis]\n",
    "    if input_image is not None and len(input_image.shape) == 3:\n",
    "        input_image = input_image[..., np.newaxis]\n",
    "    if target_image is not None and len(target_image.shape) == 3:\n",
    "        target_image = target_image[..., np.newaxis]\n",
    "    if mask_image is not None and len(mask_image.shape) == 3:\n",
    "        mask_image = mask_image[..., np.newaxis]\n",
    "    if noisy_input_image is not None and len(noisy_input_image.shape) == 3:\n",
    "        noisy_input_image = noisy_input_image[..., np.newaxis]\n",
    "    if noisy_pred_image is not None and len(noisy_pred_image.shape) == 3:\n",
    "        noisy_pred_image = noisy_pred_image[..., np.newaxis]\n",
    "    \n",
    "    # Determine z-slice to display\n",
    "    if z_slice is None:\n",
    "        z_slice = pred_image.shape[0] // 2\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Row 1: Original images (grayscale with auto-balance)\n",
    "    ax1 = fig.add_subplot(3, 3, 1)\n",
    "    if input_image is not None:\n",
    "        img_balanced = auto_balance(input_image[z_slice, :, :, 0])\n",
    "        ax1.imshow(img_balanced, cmap='gray')\n",
    "    ax1.set_title('Input Signal')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(3, 3, 2)\n",
    "    if target_image is not None:\n",
    "        img_balanced = auto_balance(target_image[z_slice, :, :, 0])\n",
    "        ax2.imshow(img_balanced, cmap='gray')\n",
    "    ax2.set_title('Target')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig.add_subplot(3, 3, 3)\n",
    "    img_balanced = auto_balance(pred_image[z_slice, :, :, 0])\n",
    "    ax3.imshow(img_balanced, cmap='gray')\n",
    "    ax3.set_title('Prediction')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Row 2: Importance mask (heatmap) and overlays\n",
    "    if mask_image is not None:\n",
    "        ax4 = fig.add_subplot(3, 3, 4)\n",
    "        mask_balanced = auto_balance(mask_image[z_slice, :, :, 0])\n",
    "        im = ax4.imshow(mask_balanced, cmap='hot')\n",
    "        ax4.set_title('Importance Mask')\n",
    "        ax4.axis('off')\n",
    "        plt.colorbar(im, ax=ax4, fraction=0.046)\n",
    "        \n",
    "        ax5 = fig.add_subplot(3, 3, 5)\n",
    "        if input_image is not None:\n",
    "            input_balanced = auto_balance(input_image[z_slice, :, :, 0])\n",
    "            mask_balanced = auto_balance(mask_image[z_slice, :, :, 0])\n",
    "            ax5.imshow(input_balanced, cmap='gray', alpha=0.7)\n",
    "            ax5.imshow(mask_balanced, cmap='Reds', alpha=0.4)\n",
    "        ax5.set_title('Input + Mask Overlay')\n",
    "        ax5.axis('off')\n",
    "        \n",
    "        ax6 = fig.add_subplot(3, 3, 6)\n",
    "        if target_image is not None:\n",
    "            target_balanced = auto_balance(target_image[z_slice, :, :, 0])\n",
    "            mask_balanced = auto_balance(mask_image[z_slice, :, :, 0])\n",
    "            ax6.imshow(target_balanced, cmap='gray', alpha=0.7)\n",
    "            ax6.imshow(mask_balanced, cmap='Reds', alpha=0.4)\n",
    "        ax6.set_title('Target + Mask Overlay')\n",
    "        ax6.axis('off')\n",
    "    \n",
    "    # Row 3: Noisy versions (grayscale with auto-balance) and difference map (heatmap)\n",
    "    if noisy_input_image is not None:\n",
    "        ax7 = fig.add_subplot(3, 3, 7)\n",
    "        img_balanced = auto_balance(noisy_input_image[z_slice, :, :, 0])\n",
    "        ax7.imshow(img_balanced, cmap='gray')\n",
    "        ax7.set_title('Noisy Input')\n",
    "        ax7.axis('off')\n",
    "    \n",
    "    if noisy_pred_image is not None:\n",
    "        ax8 = fig.add_subplot(3, 3, 8)\n",
    "        img_balanced = auto_balance(noisy_pred_image[z_slice, :, :, 0])\n",
    "        ax8.imshow(img_balanced, cmap='gray')\n",
    "        ax8.set_title('Noisy Prediction')\n",
    "        ax8.axis('off')\n",
    "    \n",
    "    # Difference map (keep as heatmap)\n",
    "    if noisy_pred_image is not None:\n",
    "        ax9 = fig.add_subplot(3, 3, 9)\n",
    "        pred_balanced = auto_balance(pred_image[z_slice, :, :, 0])\n",
    "        noisy_pred_balanced = auto_balance(noisy_pred_image[z_slice, :, :, 0])\n",
    "        diff = pred_balanced - noisy_pred_balanced\n",
    "        im_diff = ax9.imshow(diff, cmap='RdBu_r', vmin=-np.abs(diff).max(), vmax=np.abs(diff).max())\n",
    "        ax9.set_title('Prediction Difference')\n",
    "        ax9.axis('off')\n",
    "        plt.colorbar(im_diff, ax=ax9, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle(f'Image {image_index} - Z-slice {z_slice}/{pred_image.shape[0]}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_image, mask_image\n",
    "\n",
    "# Visualize results for the first image\n",
    "try:\n",
    "    pred_image, mask_image = visualize_results(\n",
    "        model_path=model_path,\n",
    "        image_index=0,\n",
    "        z_slice=19  # None for middle slice, or specify a number like 15\n",
    "    )\n",
    "    if pred_image is not None:\n",
    "        print(\"Visualization completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eda4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results from analyze_th\n",
    "results_dir = f\"{model_path}/predictions\"\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(f\"Results directory: {results_dir}\")\n",
    "    \n",
    "    # Check for results CSV files\n",
    "    pcc_results_path = f\"{results_dir}/pcc_resuls.csv\"\n",
    "    mask_size_results_path = f\"{results_dir}/mask_size_resuls.csv\"\n",
    "    \n",
    "    if os.path.exists(pcc_results_path):\n",
    "        pcc_results = pd.read_csv(pcc_results_path)\n",
    "        print(\"\\nPCC Results (Mask Efficacy):\")\n",
    "        print(pcc_results)\n",
    "    \n",
    "    if os.path.exists(mask_size_results_path):\n",
    "        mask_size_results = pd.read_csv(mask_size_results_path)\n",
    "        print(\"\\nMask Size Results:\")\n",
    "        print(mask_size_results)\n",
    "    \n",
    "    # List generated images\n",
    "    print(f\"\\nGenerated files in {results_dir}:\")\n",
    "    for item in os.listdir(results_dir):\n",
    "        item_path = os.path.join(results_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  [DIR] {item}\")\n",
    "        else:\n",
    "            print(f\"  [FILE] {item}\")\n",
    "else:\n",
    "    print(f\"Results directory not found: {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskinterpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
